{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel Intro Tutorial: Data Slicing\n",
    "Traditional machine learning systems optimize for overall quality, which may be too coarse-grained. Models that achieve high overall performance might produce unacceptable failure rates on critical slices of the data.\n",
    "\n",
    "*Note:* tutorial differs from the labeling tutorial in that we use ground truth labels in the train split for demo purposes. SFs are intended to be used after the training set has already been labeled by LFs (or by hand) in the training data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_spam_dataset\n",
    "\n",
    "df_train, df_test = load_spam_dataset(load_train_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Writing slicing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By leveraging **slicing functions (SFs)**, which output binary masks indicating whether an data point is in the slice of not. Each slice represents some noisily-defined subset of the data (corresponding to an SF) that we'd like to programmatically monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from snorkel.slicing import slicing_function\n",
    "\n",
    "@slicing_function()\n",
    "def short_comment(x):\n",
    "  # HAM comments are often short, such as 'cool video!'\n",
    "  return len(x.text.split()) < 5\n",
    "\n",
    "sfs = [short_comment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 65307.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.slicing import slice_dataframe\n",
    "\n",
    "short_comment_df = slice_dataframe(df_test, short_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>super music﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I like shakira..﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>subscribe to my feed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Awesome ﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Nice</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      text  label\n",
       "194           super music﻿      0\n",
       "2        I like shakira..﻿      0\n",
       "110   subscribe to my feed      1\n",
       "263              Awesome ﻿      0\n",
       "77                    Nice      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_comment_df[[\"text\", \"label\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monitor slice performance with `Scorer.score_slices`\n",
    "\n",
    "### Train a simple classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from utils import df_to_features\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X_train, Y_train = df_to_features(vectorizer, df_train, \"train\")\n",
    "X_test, Y_test = df_to_features(vectorizer, df_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, solver='liblinear')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sklearn_model = LogisticRegression(C=0.001, solver=\"liblinear\")\n",
    "sklearn_model.fit(X=X_train, y=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, solver='liblinear')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression(\n",
    "  C=0.001, \n",
    "  class_weight=None,\n",
    "  dual=False,\n",
    "  fit_intercept=True,\n",
    "  intercept_scaling=1,\n",
    "  l1_ratio=None,\n",
    "  max_iter=100,\n",
    "  multi_class=\"auto\",\n",
    "  n_jobs=None,\n",
    "  penalty=\"l2\",\n",
    "  random_state=None,\n",
    "  solver=\"liblinear\",\n",
    "  tol=0.0001,\n",
    "  verbose=0,\n",
    "  warm_start=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.utils import preds_to_probs\n",
    "\n",
    "preds_test = sklearn_model.predict(X_test)\n",
    "probs_test = preds_to_probs(preds_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1: 92.5%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f\"Test set F1: {100 * f1_score(Y_test, preds_test):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store slice metadata in `S`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 89852.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.slicing import PandasSFApplier\n",
    "\n",
    "applier = PandasSFApplier(sfs)\n",
    "S_test = applier.apply(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.analysis import Scorer\n",
    "\n",
    "scorer = Scorer(metrics=[\"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall</th>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     f1\n",
       "overall        0.925000\n",
       "short_comment  0.666667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score_slices(\n",
    "  S=S_test,\n",
    "  golds=Y_test,\n",
    "  preds=preds_test,\n",
    "  probs=probs_test,\n",
    "  as_dataframe=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite high overall performance, the ` short_comment` slice performs poorly here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing additonal slicing functions (SFs)\n",
    "\n",
    "Slices are dynamic - as monitoring needs grow or change with new data distributions or application needs, an ML pipeline might require dozens, or even hundreds, of slices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.slicing import SlicingFunction, slicing_function\n",
    "from snorkel.preprocess import preprocessor\n",
    "\n",
    "\n",
    "# Keyword-based SFs\n",
    "def keyword_lookup(x, keywords):\n",
    "  return any(word in x.text.lower() for word in keywords)\n",
    "\n",
    "def make_keyword_sf(keywords):\n",
    "  return SlicingFunction(\n",
    "    name=f\"keyword_{keywords[0]}\",\n",
    "    f=keyword_lookup,\n",
    "    resources=dict(keywords=keywords)\n",
    "  )\n",
    "\n",
    "keyword_please = make_keyword_sf(keywords=[\"please\", \"plz\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex-based SFs\n",
    "@slicing_function()\n",
    "def regex_check_out(x):\n",
    "  return bool(re.search(r\"check.*out\", x.text, flags=re.I))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@slicing_function()\n",
    "def short_link(x):\n",
    "  return bool(re.search(r\"\\w+\\.ly\", x.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def textblob_sentiment(x):\n",
    "  scores = TextBlob(x.text)\n",
    "  x.polarity = scores.sentiment.polarity\n",
    "  return x\n",
    "\n",
    "@slicing_function(pre=[textblob_sentiment])\n",
    "def textblob_polarity(x):\n",
    "  return x.polarity > 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 2965.84it/s]\n"
     ]
    }
   ],
   "source": [
    "polarity_df = slice_dataframe(df_test, textblob_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Awesome ﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>OMG LISTEN TO THIS ITS SOO GOOD!! :D﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Shakira is very beautiful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>awesome</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text  label\n",
       "263                              Awesome ﻿      0\n",
       "240             Shakira is the best dancer      0\n",
       "261  OMG LISTEN TO THIS ITS SOO GOOD!! :D﻿      0\n",
       "14               Shakira is very beautiful      0\n",
       "114                                awesome      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_df[[\"text\", \"label\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_sfs = [\n",
    "  keyword_please,\n",
    "  regex_check_out,\n",
    "  short_link,\n",
    "  textblob_polarity\n",
    "]\n",
    "\n",
    "sfs = [short_comment] + extra_sfs\n",
    "slice_names = [sf.name for sf in sfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 25784.44it/s]\n"
     ]
    }
   ],
   "source": [
    "applier = PandasSFApplier(sfs)\n",
    "S_test = applier.apply(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall</th>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_comment</th>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_please</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regex_check_out</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_link</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_polarity</th>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         f1\n",
       "overall            0.925000\n",
       "short_comment      0.666667\n",
       "keyword_please     1.000000\n",
       "regex_check_out    1.000000\n",
       "short_link         0.500000\n",
       "textblob_polarity  0.727273"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score_slices(\n",
    "    S=S_test, \n",
    "    golds=Y_test, \n",
    "    preds=preds_test, \n",
    "    probs=probs_test, \n",
    "    as_dataframe=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improve slice performance\n",
    "This section will demonstrate a modeling approach that we call **Slice-based Learning**, which improves performance by adding extra slice-specific representational capacity to whichever model we're using. Intuitively, we'd like to model to learn representations that are better suited to handle data points in *specific slice*. The approach is to model each slice as a seperate \"expert task\" in the style of multi-task learning;\n",
    "\n",
    "In other approaches, one might attemp to increase slice performance with techniques like *oversampling* (i.e. with PyTorch's `WeightedRandomSampler`), effectively shifting the training distribution towards certain populations.\n",
    "\n",
    "This might work with small number of slices, but with hundreds or thousands or production slices at scale, it could quickly become intractable to tune upsampling weights per slice.\n",
    "\n",
    "### Constructing a `SliceAwareClassifier`\n",
    "\n",
    "To cope with scale, we will attemp to learn and combine many slice-specific representations with an attention mechanism.\n",
    "\n",
    "First initialize a `SliceAwareClassifier`\n",
    "- `base_architecture`: Defines a simple Multi-Layer Perceptron (MLP) in PyTorch to serve as the primary representation architecture. Noted that the `BinarySlicingClassifier` is **agnostic to the base architecture** - can leverage a transformer model for text, or a ResNet for images.\n",
    "- `head_dim`: identifies the final output feature dimension of the `base_architecture`\n",
    "- `slice_names`: specify the slices that we plan to train on with this classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.slicing import SliceAwareClassifier\n",
    "from utils import get_pytorch_mlp\n",
    "\n",
    "# Define model architecture\n",
    "bow_dim = X_train.shape[1]\n",
    "hidden_dim = bow_dim\n",
    "mlp = get_pytorch_mlp(hidden_dim=hidden_dim, num_layers=2)\n",
    "\n",
    "# Initialize slice model\n",
    "slice_model = SliceAwareClassifier(\n",
    "  base_architecture=mlp,\n",
    "  head_dim=hidden_dim,\n",
    "  slice_names=[sf.name for sf in sfs],\n",
    "  scorer=scorer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1586/1586 [00:00<00:00, 3854.60it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 27118.11it/s]\n"
     ]
    }
   ],
   "source": [
    "applier = PandasSFApplier(sfs)\n",
    "S_train = applier.apply(df_train)\n",
    "S_test = applier.apply(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train using slice information, we'd like to initialize a **slice-aware dataloader**. We can use `slice_model.make_slice_dataloader` to add slice labels to an existing dataloader.\n",
    "\n",
    "Under the hood, this method leverages slice metadata to add slice labels to the appropriate fields such that it will be compatible with our model, a `SliceAwareClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_dict_dataloader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dl = create_dict_dataloader(X_train, Y_train, \"train\")\n",
    "train_dl_slice = slice_model.make_slice_dataloader(\n",
    "  train_dl.dataset, S_train, shuffle=True, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "test_dl = create_dict_dataloader(X_test, Y_test, \"train\")\n",
    "test_dl_slice = slice_model.make_slice_dataloader(\n",
    "  test_dl.dataset, S_test, shuffle=False, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation learning with slices\n",
    "Using Snorkel's `Trainer`, we fit our classifier with the training set dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:: 100%|██████████| 25/25 [00:06<00:00,  3.98it/s, model/all/train/loss=0.504, model/all/train/lr=0.0001]\n",
      "Epoch 1:: 100%|██████████| 25/25 [00:06<00:00,  4.05it/s, model/all/train/loss=0.269, model/all/train/lr=0.0001]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.classification import Trainer\n",
    "\n",
    "# For demostration purpose, we set n_epochs = 2\n",
    "trainer = Trainer(n_epochs=2, lr=1e-4, progress_bar=True)\n",
    "trainer.fit(slice_model, [train_dl_slice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At inference time, the primary task head (`spam_task`) will make all final predictions. We'd like to evaluate all the slice heads on the original task head - `score_slices` remaps all slice-related labels, denoted `spam_task_slice:{slice_name}_pred`, to be evaluated on the `spam_task`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Reading\n",
    "\n",
    "- [Multi-task learning](https://github.com/snorkel-team/snorkel-tutorials/blob/master/multitask/multitask_tutorial.ipynb)\n",
    "- [Slice-based Learning Code Example](https://github.com/snorkel-team/snorkel/blob/master/snorkel/slicing/utils.py)\n",
    "- [Slice-based Learning: A Programming Model for Residual Learning in Critical Data Slices](https://arxiv.org/abs/1909.06349)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23ed3cb3e99994661ae26b1d54a1c2169c20f60209a6034de1ca790bb120c66d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('snorkel': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
